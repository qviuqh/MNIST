{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d66dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# Tải dữ liệu\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Tiền xử lý\n",
    "x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n",
    "x_test  = x_test.reshape(-1, 784).astype('float32') / 255.0\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test  = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "766fc7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # chống overflow\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    return -np.sum(y_true * np.log(y_pred + 1e-8)) / m\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eea077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n",
    "        # Khởi tạo trọng số và bias\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.z1 = np.dot(x, self.W1) + self.b1\n",
    "        self.a1 = relu(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = softmax(self.z2)\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, y_true):\n",
    "        m = y_true.shape[0]\n",
    "        dz2 = self.a2 - y_true\n",
    "        dW2 = np.dot(self.a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "\n",
    "        da1 = np.dot(dz2, self.W2.T)\n",
    "        dz1 = da1 * relu_derivative(self.z1)\n",
    "        dW1 = np.dot(self.x.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "\n",
    "        # Cập nhật\n",
    "        self.W1 -= self.lr * dW1\n",
    "        self.b1 -= self.lr * db1\n",
    "        self.W2 -= self.lr * dW2\n",
    "        self.b2 -= self.lr * db2\n",
    "\n",
    "    def train(self, x_train, y_train, x_test, y_test, epochs=20):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(x_train)\n",
    "            loss = cross_entropy_loss(y_train, y_pred)\n",
    "            self.backward(y_train)\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                test_pred = self.forward(x_test)\n",
    "                acc = accuracy(y_test, test_pred)\n",
    "                print(f\"Epoch {epoch+1:02d}: Loss = {loss:.4f}, Test Accuracy = {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c9ee16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Loss = 2.3026, Test Accuracy = 0.1359\n",
      "Epoch 02: Loss = 2.2995, Test Accuracy = 0.2433\n",
      "Epoch 03: Loss = 2.2964, Test Accuracy = 0.3858\n",
      "Epoch 04: Loss = 2.2932, Test Accuracy = 0.4738\n",
      "Epoch 05: Loss = 2.2895, Test Accuracy = 0.5141\n",
      "Epoch 06: Loss = 2.2853, Test Accuracy = 0.5392\n",
      "Epoch 07: Loss = 2.2804, Test Accuracy = 0.5497\n",
      "Epoch 08: Loss = 2.2745, Test Accuracy = 0.5537\n",
      "Epoch 09: Loss = 2.2674, Test Accuracy = 0.5504\n",
      "Epoch 10: Loss = 2.2590, Test Accuracy = 0.5441\n",
      "Epoch 11: Loss = 2.2488, Test Accuracy = 0.5376\n",
      "Epoch 12: Loss = 2.2367, Test Accuracy = 0.5317\n",
      "Epoch 13: Loss = 2.2222, Test Accuracy = 0.5272\n",
      "Epoch 14: Loss = 2.2051, Test Accuracy = 0.5264\n",
      "Epoch 15: Loss = 2.1849, Test Accuracy = 0.5285\n",
      "Epoch 16: Loss = 2.1613, Test Accuracy = 0.5377\n",
      "Epoch 17: Loss = 2.1338, Test Accuracy = 0.5498\n",
      "Epoch 18: Loss = 2.1022, Test Accuracy = 0.5669\n",
      "Epoch 19: Loss = 2.0661, Test Accuracy = 0.5834\n",
      "Epoch 20: Loss = 2.0254, Test Accuracy = 0.6053\n",
      "Epoch 21: Loss = 1.9798, Test Accuracy = 0.6279\n",
      "Epoch 22: Loss = 1.9294, Test Accuracy = 0.6460\n",
      "Epoch 23: Loss = 1.8743, Test Accuracy = 0.6629\n",
      "Epoch 24: Loss = 1.8150, Test Accuracy = 0.6772\n",
      "Epoch 25: Loss = 1.7520, Test Accuracy = 0.6892\n",
      "Epoch 26: Loss = 1.6862, Test Accuracy = 0.6967\n",
      "Epoch 27: Loss = 1.6188, Test Accuracy = 0.7067\n",
      "Epoch 28: Loss = 1.5509, Test Accuracy = 0.7162\n",
      "Epoch 29: Loss = 1.4838, Test Accuracy = 0.7235\n",
      "Epoch 30: Loss = 1.4185, Test Accuracy = 0.7313\n",
      "Epoch 31: Loss = 1.3560, Test Accuracy = 0.7400\n",
      "Epoch 32: Loss = 1.2969, Test Accuracy = 0.7504\n",
      "Epoch 33: Loss = 1.2415, Test Accuracy = 0.7588\n",
      "Epoch 34: Loss = 1.1899, Test Accuracy = 0.7682\n",
      "Epoch 35: Loss = 1.1421, Test Accuracy = 0.7756\n",
      "Epoch 36: Loss = 1.0979, Test Accuracy = 0.7822\n",
      "Epoch 37: Loss = 1.0572, Test Accuracy = 0.7883\n",
      "Epoch 38: Loss = 1.0196, Test Accuracy = 0.7931\n",
      "Epoch 39: Loss = 0.9850, Test Accuracy = 0.7973\n",
      "Epoch 40: Loss = 0.9529, Test Accuracy = 0.7996\n",
      "Epoch 41: Loss = 0.9233, Test Accuracy = 0.8049\n",
      "Epoch 42: Loss = 0.8959, Test Accuracy = 0.8075\n",
      "Epoch 43: Loss = 0.8705, Test Accuracy = 0.8106\n",
      "Epoch 44: Loss = 0.8469, Test Accuracy = 0.8141\n",
      "Epoch 45: Loss = 0.8249, Test Accuracy = 0.8175\n",
      "Epoch 46: Loss = 0.8044, Test Accuracy = 0.8205\n",
      "Epoch 47: Loss = 0.7853, Test Accuracy = 0.8237\n",
      "Epoch 48: Loss = 0.7674, Test Accuracy = 0.8270\n",
      "Epoch 49: Loss = 0.7507, Test Accuracy = 0.8289\n",
      "Epoch 50: Loss = 0.7350, Test Accuracy = 0.8315\n",
      "Epoch 51: Loss = 0.7203, Test Accuracy = 0.8335\n",
      "Epoch 52: Loss = 0.7064, Test Accuracy = 0.8365\n",
      "Epoch 53: Loss = 0.6933, Test Accuracy = 0.8396\n",
      "Epoch 54: Loss = 0.6810, Test Accuracy = 0.8423\n",
      "Epoch 55: Loss = 0.6693, Test Accuracy = 0.8438\n",
      "Epoch 56: Loss = 0.6583, Test Accuracy = 0.8456\n",
      "Epoch 57: Loss = 0.6478, Test Accuracy = 0.8472\n",
      "Epoch 58: Loss = 0.6379, Test Accuracy = 0.8497\n",
      "Epoch 59: Loss = 0.6285, Test Accuracy = 0.8512\n",
      "Epoch 60: Loss = 0.6195, Test Accuracy = 0.8530\n",
      "Epoch 61: Loss = 0.6110, Test Accuracy = 0.8538\n",
      "Epoch 62: Loss = 0.6028, Test Accuracy = 0.8561\n",
      "Epoch 63: Loss = 0.5950, Test Accuracy = 0.8574\n",
      "Epoch 64: Loss = 0.5876, Test Accuracy = 0.8585\n",
      "Epoch 65: Loss = 0.5805, Test Accuracy = 0.8600\n",
      "Epoch 66: Loss = 0.5737, Test Accuracy = 0.8610\n",
      "Epoch 67: Loss = 0.5671, Test Accuracy = 0.8624\n",
      "Epoch 68: Loss = 0.5609, Test Accuracy = 0.8638\n",
      "Epoch 69: Loss = 0.5548, Test Accuracy = 0.8647\n",
      "Epoch 70: Loss = 0.5491, Test Accuracy = 0.8655\n",
      "Epoch 71: Loss = 0.5435, Test Accuracy = 0.8669\n",
      "Epoch 72: Loss = 0.5381, Test Accuracy = 0.8676\n",
      "Epoch 73: Loss = 0.5330, Test Accuracy = 0.8691\n",
      "Epoch 74: Loss = 0.5280, Test Accuracy = 0.8698\n",
      "Epoch 75: Loss = 0.5232, Test Accuracy = 0.8704\n",
      "Epoch 76: Loss = 0.5186, Test Accuracy = 0.8722\n",
      "Epoch 77: Loss = 0.5141, Test Accuracy = 0.8732\n",
      "Epoch 78: Loss = 0.5098, Test Accuracy = 0.8740\n",
      "Epoch 79: Loss = 0.5056, Test Accuracy = 0.8752\n",
      "Epoch 80: Loss = 0.5016, Test Accuracy = 0.8755\n",
      "Epoch 81: Loss = 0.4977, Test Accuracy = 0.8763\n",
      "Epoch 82: Loss = 0.4939, Test Accuracy = 0.8769\n",
      "Epoch 83: Loss = 0.4902, Test Accuracy = 0.8782\n",
      "Epoch 84: Loss = 0.4866, Test Accuracy = 0.8790\n",
      "Epoch 85: Loss = 0.4832, Test Accuracy = 0.8798\n",
      "Epoch 86: Loss = 0.4798, Test Accuracy = 0.8804\n",
      "Epoch 87: Loss = 0.4766, Test Accuracy = 0.8811\n",
      "Epoch 88: Loss = 0.4734, Test Accuracy = 0.8821\n",
      "Epoch 89: Loss = 0.4704, Test Accuracy = 0.8826\n",
      "Epoch 90: Loss = 0.4674, Test Accuracy = 0.8829\n",
      "Epoch 91: Loss = 0.4645, Test Accuracy = 0.8842\n",
      "Epoch 92: Loss = 0.4617, Test Accuracy = 0.8847\n",
      "Epoch 93: Loss = 0.4589, Test Accuracy = 0.8853\n",
      "Epoch 94: Loss = 0.4563, Test Accuracy = 0.8853\n",
      "Epoch 95: Loss = 0.4537, Test Accuracy = 0.8860\n",
      "Epoch 96: Loss = 0.4511, Test Accuracy = 0.8866\n",
      "Epoch 97: Loss = 0.4487, Test Accuracy = 0.8870\n",
      "Epoch 98: Loss = 0.4463, Test Accuracy = 0.8877\n",
      "Epoch 99: Loss = 0.4439, Test Accuracy = 0.8873\n",
      "Epoch 100: Loss = 0.4416, Test Accuracy = 0.8878\n",
      "Epoch 101: Loss = 0.4394, Test Accuracy = 0.8880\n",
      "Epoch 102: Loss = 0.4372, Test Accuracy = 0.8883\n",
      "Epoch 103: Loss = 0.4351, Test Accuracy = 0.8886\n",
      "Epoch 104: Loss = 0.4331, Test Accuracy = 0.8890\n",
      "Epoch 105: Loss = 0.4310, Test Accuracy = 0.8898\n",
      "Epoch 106: Loss = 0.4291, Test Accuracy = 0.8902\n",
      "Epoch 107: Loss = 0.4271, Test Accuracy = 0.8906\n",
      "Epoch 108: Loss = 0.4252, Test Accuracy = 0.8910\n",
      "Epoch 109: Loss = 0.4234, Test Accuracy = 0.8915\n",
      "Epoch 110: Loss = 0.4216, Test Accuracy = 0.8916\n",
      "Epoch 111: Loss = 0.4198, Test Accuracy = 0.8920\n",
      "Epoch 112: Loss = 0.4181, Test Accuracy = 0.8922\n",
      "Epoch 113: Loss = 0.4164, Test Accuracy = 0.8927\n",
      "Epoch 114: Loss = 0.4148, Test Accuracy = 0.8929\n",
      "Epoch 115: Loss = 0.4131, Test Accuracy = 0.8931\n",
      "Epoch 116: Loss = 0.4116, Test Accuracy = 0.8936\n",
      "Epoch 117: Loss = 0.4100, Test Accuracy = 0.8938\n",
      "Epoch 118: Loss = 0.4085, Test Accuracy = 0.8941\n",
      "Epoch 119: Loss = 0.4070, Test Accuracy = 0.8944\n",
      "Epoch 120: Loss = 0.4055, Test Accuracy = 0.8946\n",
      "Epoch 121: Loss = 0.4041, Test Accuracy = 0.8950\n",
      "Epoch 122: Loss = 0.4027, Test Accuracy = 0.8950\n",
      "Epoch 123: Loss = 0.4013, Test Accuracy = 0.8951\n",
      "Epoch 124: Loss = 0.3999, Test Accuracy = 0.8955\n",
      "Epoch 125: Loss = 0.3986, Test Accuracy = 0.8958\n",
      "Epoch 126: Loss = 0.3973, Test Accuracy = 0.8958\n",
      "Epoch 127: Loss = 0.3960, Test Accuracy = 0.8963\n",
      "Epoch 128: Loss = 0.3948, Test Accuracy = 0.8965\n",
      "Epoch 129: Loss = 0.3935, Test Accuracy = 0.8969\n",
      "Epoch 130: Loss = 0.3923, Test Accuracy = 0.8971\n",
      "Epoch 131: Loss = 0.3911, Test Accuracy = 0.8973\n",
      "Epoch 132: Loss = 0.3899, Test Accuracy = 0.8974\n",
      "Epoch 133: Loss = 0.3888, Test Accuracy = 0.8977\n",
      "Epoch 134: Loss = 0.3877, Test Accuracy = 0.8979\n",
      "Epoch 135: Loss = 0.3865, Test Accuracy = 0.8980\n",
      "Epoch 136: Loss = 0.3854, Test Accuracy = 0.8983\n",
      "Epoch 137: Loss = 0.3844, Test Accuracy = 0.8985\n",
      "Epoch 138: Loss = 0.3833, Test Accuracy = 0.8988\n",
      "Epoch 139: Loss = 0.3823, Test Accuracy = 0.8989\n",
      "Epoch 140: Loss = 0.3812, Test Accuracy = 0.8992\n",
      "Epoch 141: Loss = 0.3802, Test Accuracy = 0.8992\n",
      "Epoch 142: Loss = 0.3792, Test Accuracy = 0.8992\n",
      "Epoch 143: Loss = 0.3782, Test Accuracy = 0.8996\n",
      "Epoch 144: Loss = 0.3773, Test Accuracy = 0.9002\n",
      "Epoch 145: Loss = 0.3763, Test Accuracy = 0.9005\n",
      "Epoch 146: Loss = 0.3754, Test Accuracy = 0.9010\n",
      "Epoch 147: Loss = 0.3745, Test Accuracy = 0.9013\n",
      "Epoch 148: Loss = 0.3735, Test Accuracy = 0.9010\n",
      "Epoch 149: Loss = 0.3726, Test Accuracy = 0.9012\n",
      "Epoch 150: Loss = 0.3718, Test Accuracy = 0.9015\n",
      "Epoch 151: Loss = 0.3709, Test Accuracy = 0.9016\n",
      "Epoch 152: Loss = 0.3700, Test Accuracy = 0.9015\n",
      "Epoch 153: Loss = 0.3692, Test Accuracy = 0.9019\n",
      "Epoch 154: Loss = 0.3683, Test Accuracy = 0.9021\n",
      "Epoch 155: Loss = 0.3675, Test Accuracy = 0.9024\n",
      "Epoch 156: Loss = 0.3667, Test Accuracy = 0.9025\n",
      "Epoch 157: Loss = 0.3659, Test Accuracy = 0.9029\n",
      "Epoch 158: Loss = 0.3651, Test Accuracy = 0.9031\n",
      "Epoch 159: Loss = 0.3643, Test Accuracy = 0.9035\n",
      "Epoch 160: Loss = 0.3635, Test Accuracy = 0.9038\n",
      "Epoch 161: Loss = 0.3628, Test Accuracy = 0.9041\n",
      "Epoch 162: Loss = 0.3620, Test Accuracy = 0.9043\n",
      "Epoch 163: Loss = 0.3612, Test Accuracy = 0.9043\n",
      "Epoch 164: Loss = 0.3605, Test Accuracy = 0.9046\n",
      "Epoch 165: Loss = 0.3598, Test Accuracy = 0.9048\n",
      "Epoch 166: Loss = 0.3591, Test Accuracy = 0.9047\n",
      "Epoch 167: Loss = 0.3583, Test Accuracy = 0.9048\n",
      "Epoch 168: Loss = 0.3576, Test Accuracy = 0.9050\n",
      "Epoch 169: Loss = 0.3569, Test Accuracy = 0.9050\n",
      "Epoch 170: Loss = 0.3563, Test Accuracy = 0.9052\n",
      "Epoch 171: Loss = 0.3556, Test Accuracy = 0.9053\n",
      "Epoch 172: Loss = 0.3549, Test Accuracy = 0.9054\n",
      "Epoch 173: Loss = 0.3542, Test Accuracy = 0.9055\n",
      "Epoch 174: Loss = 0.3536, Test Accuracy = 0.9055\n",
      "Epoch 175: Loss = 0.3529, Test Accuracy = 0.9057\n",
      "Epoch 176: Loss = 0.3523, Test Accuracy = 0.9060\n",
      "Epoch 177: Loss = 0.3516, Test Accuracy = 0.9060\n",
      "Epoch 178: Loss = 0.3510, Test Accuracy = 0.9061\n",
      "Epoch 179: Loss = 0.3504, Test Accuracy = 0.9062\n",
      "Epoch 180: Loss = 0.3498, Test Accuracy = 0.9063\n",
      "Epoch 181: Loss = 0.3491, Test Accuracy = 0.9060\n",
      "Epoch 182: Loss = 0.3485, Test Accuracy = 0.9062\n",
      "Epoch 183: Loss = 0.3479, Test Accuracy = 0.9063\n",
      "Epoch 184: Loss = 0.3473, Test Accuracy = 0.9063\n",
      "Epoch 185: Loss = 0.3468, Test Accuracy = 0.9063\n",
      "Epoch 186: Loss = 0.3462, Test Accuracy = 0.9064\n",
      "Epoch 187: Loss = 0.3456, Test Accuracy = 0.9065\n",
      "Epoch 188: Loss = 0.3450, Test Accuracy = 0.9066\n",
      "Epoch 189: Loss = 0.3445, Test Accuracy = 0.9068\n",
      "Epoch 190: Loss = 0.3439, Test Accuracy = 0.9069\n",
      "Epoch 191: Loss = 0.3433, Test Accuracy = 0.9071\n",
      "Epoch 192: Loss = 0.3428, Test Accuracy = 0.9070\n",
      "Epoch 193: Loss = 0.3422, Test Accuracy = 0.9072\n",
      "Epoch 194: Loss = 0.3417, Test Accuracy = 0.9073\n",
      "Epoch 195: Loss = 0.3412, Test Accuracy = 0.9073\n",
      "Epoch 196: Loss = 0.3406, Test Accuracy = 0.9075\n",
      "Epoch 197: Loss = 0.3401, Test Accuracy = 0.9075\n",
      "Epoch 198: Loss = 0.3396, Test Accuracy = 0.9076\n",
      "Epoch 199: Loss = 0.3391, Test Accuracy = 0.9079\n",
      "Epoch 200: Loss = 0.3385, Test Accuracy = 0.9080\n",
      "Epoch 201: Loss = 0.3380, Test Accuracy = 0.9081\n",
      "Epoch 202: Loss = 0.3375, Test Accuracy = 0.9083\n",
      "Epoch 203: Loss = 0.3370, Test Accuracy = 0.9085\n",
      "Epoch 204: Loss = 0.3365, Test Accuracy = 0.9085\n",
      "Epoch 205: Loss = 0.3360, Test Accuracy = 0.9086\n",
      "Epoch 206: Loss = 0.3355, Test Accuracy = 0.9089\n",
      "Epoch 207: Loss = 0.3350, Test Accuracy = 0.9090\n",
      "Epoch 208: Loss = 0.3346, Test Accuracy = 0.9092\n",
      "Epoch 209: Loss = 0.3341, Test Accuracy = 0.9091\n",
      "Epoch 210: Loss = 0.3336, Test Accuracy = 0.9090\n",
      "Epoch 211: Loss = 0.3331, Test Accuracy = 0.9090\n",
      "Epoch 212: Loss = 0.3327, Test Accuracy = 0.9092\n",
      "Epoch 213: Loss = 0.3322, Test Accuracy = 0.9094\n",
      "Epoch 214: Loss = 0.3317, Test Accuracy = 0.9095\n",
      "Epoch 215: Loss = 0.3313, Test Accuracy = 0.9096\n",
      "Epoch 216: Loss = 0.3308, Test Accuracy = 0.9097\n",
      "Epoch 217: Loss = 0.3304, Test Accuracy = 0.9098\n",
      "Epoch 218: Loss = 0.3299, Test Accuracy = 0.9099\n",
      "Epoch 219: Loss = 0.3295, Test Accuracy = 0.9099\n",
      "Epoch 220: Loss = 0.3290, Test Accuracy = 0.9102\n",
      "Epoch 221: Loss = 0.3286, Test Accuracy = 0.9104\n",
      "Epoch 222: Loss = 0.3282, Test Accuracy = 0.9105\n",
      "Epoch 223: Loss = 0.3277, Test Accuracy = 0.9106\n",
      "Epoch 224: Loss = 0.3273, Test Accuracy = 0.9106\n",
      "Epoch 225: Loss = 0.3269, Test Accuracy = 0.9107\n",
      "Epoch 226: Loss = 0.3264, Test Accuracy = 0.9107\n",
      "Epoch 227: Loss = 0.3260, Test Accuracy = 0.9109\n",
      "Epoch 228: Loss = 0.3256, Test Accuracy = 0.9112\n",
      "Epoch 229: Loss = 0.3252, Test Accuracy = 0.9113\n",
      "Epoch 230: Loss = 0.3248, Test Accuracy = 0.9114\n",
      "Epoch 231: Loss = 0.3243, Test Accuracy = 0.9115\n",
      "Epoch 232: Loss = 0.3239, Test Accuracy = 0.9116\n",
      "Epoch 233: Loss = 0.3235, Test Accuracy = 0.9117\n",
      "Epoch 234: Loss = 0.3231, Test Accuracy = 0.9120\n",
      "Epoch 235: Loss = 0.3227, Test Accuracy = 0.9120\n",
      "Epoch 236: Loss = 0.3223, Test Accuracy = 0.9122\n",
      "Epoch 237: Loss = 0.3219, Test Accuracy = 0.9123\n",
      "Epoch 238: Loss = 0.3215, Test Accuracy = 0.9127\n",
      "Epoch 239: Loss = 0.3211, Test Accuracy = 0.9128\n",
      "Epoch 240: Loss = 0.3207, Test Accuracy = 0.9128\n",
      "Epoch 241: Loss = 0.3203, Test Accuracy = 0.9128\n",
      "Epoch 242: Loss = 0.3199, Test Accuracy = 0.9129\n",
      "Epoch 243: Loss = 0.3196, Test Accuracy = 0.9130\n",
      "Epoch 244: Loss = 0.3192, Test Accuracy = 0.9130\n",
      "Epoch 245: Loss = 0.3188, Test Accuracy = 0.9132\n",
      "Epoch 246: Loss = 0.3184, Test Accuracy = 0.9133\n",
      "Epoch 247: Loss = 0.3180, Test Accuracy = 0.9134\n",
      "Epoch 248: Loss = 0.3177, Test Accuracy = 0.9135\n",
      "Epoch 249: Loss = 0.3173, Test Accuracy = 0.9135\n",
      "Epoch 250: Loss = 0.3169, Test Accuracy = 0.9137\n",
      "Epoch 251: Loss = 0.3165, Test Accuracy = 0.9138\n",
      "Epoch 252: Loss = 0.3162, Test Accuracy = 0.9140\n",
      "Epoch 253: Loss = 0.3158, Test Accuracy = 0.9141\n",
      "Epoch 254: Loss = 0.3154, Test Accuracy = 0.9143\n",
      "Epoch 255: Loss = 0.3151, Test Accuracy = 0.9142\n",
      "Epoch 256: Loss = 0.3147, Test Accuracy = 0.9143\n",
      "Epoch 257: Loss = 0.3144, Test Accuracy = 0.9143\n",
      "Epoch 258: Loss = 0.3140, Test Accuracy = 0.9146\n",
      "Epoch 259: Loss = 0.3136, Test Accuracy = 0.9148\n",
      "Epoch 260: Loss = 0.3133, Test Accuracy = 0.9149\n",
      "Epoch 261: Loss = 0.3129, Test Accuracy = 0.9150\n",
      "Epoch 262: Loss = 0.3126, Test Accuracy = 0.9151\n",
      "Epoch 263: Loss = 0.3122, Test Accuracy = 0.9153\n",
      "Epoch 264: Loss = 0.3119, Test Accuracy = 0.9153\n",
      "Epoch 265: Loss = 0.3115, Test Accuracy = 0.9158\n",
      "Epoch 266: Loss = 0.3112, Test Accuracy = 0.9159\n",
      "Epoch 267: Loss = 0.3109, Test Accuracy = 0.9161\n",
      "Epoch 268: Loss = 0.3105, Test Accuracy = 0.9163\n",
      "Epoch 269: Loss = 0.3102, Test Accuracy = 0.9163\n",
      "Epoch 270: Loss = 0.3098, Test Accuracy = 0.9163\n",
      "Epoch 271: Loss = 0.3095, Test Accuracy = 0.9163\n",
      "Epoch 272: Loss = 0.3092, Test Accuracy = 0.9162\n",
      "Epoch 273: Loss = 0.3088, Test Accuracy = 0.9165\n",
      "Epoch 274: Loss = 0.3085, Test Accuracy = 0.9166\n",
      "Epoch 275: Loss = 0.3082, Test Accuracy = 0.9166\n",
      "Epoch 276: Loss = 0.3078, Test Accuracy = 0.9167\n",
      "Epoch 277: Loss = 0.3075, Test Accuracy = 0.9171\n",
      "Epoch 278: Loss = 0.3072, Test Accuracy = 0.9171\n",
      "Epoch 279: Loss = 0.3068, Test Accuracy = 0.9171\n",
      "Epoch 280: Loss = 0.3065, Test Accuracy = 0.9172\n",
      "Epoch 281: Loss = 0.3062, Test Accuracy = 0.9174\n",
      "Epoch 282: Loss = 0.3059, Test Accuracy = 0.9174\n",
      "Epoch 283: Loss = 0.3055, Test Accuracy = 0.9175\n",
      "Epoch 284: Loss = 0.3052, Test Accuracy = 0.9177\n",
      "Epoch 285: Loss = 0.3049, Test Accuracy = 0.9177\n",
      "Epoch 286: Loss = 0.3046, Test Accuracy = 0.9177\n",
      "Epoch 287: Loss = 0.3042, Test Accuracy = 0.9177\n",
      "Epoch 288: Loss = 0.3039, Test Accuracy = 0.9179\n",
      "Epoch 289: Loss = 0.3036, Test Accuracy = 0.9179\n",
      "Epoch 290: Loss = 0.3033, Test Accuracy = 0.9181\n",
      "Epoch 291: Loss = 0.3030, Test Accuracy = 0.9182\n",
      "Epoch 292: Loss = 0.3027, Test Accuracy = 0.9181\n",
      "Epoch 293: Loss = 0.3024, Test Accuracy = 0.9181\n",
      "Epoch 294: Loss = 0.3020, Test Accuracy = 0.9182\n",
      "Epoch 295: Loss = 0.3017, Test Accuracy = 0.9182\n",
      "Epoch 296: Loss = 0.3014, Test Accuracy = 0.9182\n",
      "Epoch 297: Loss = 0.3011, Test Accuracy = 0.9185\n",
      "Epoch 298: Loss = 0.3008, Test Accuracy = 0.9186\n",
      "Epoch 299: Loss = 0.3005, Test Accuracy = 0.9189\n",
      "Epoch 300: Loss = 0.3002, Test Accuracy = 0.9189\n"
     ]
    }
   ],
   "source": [
    "# Khởi tạo và huấn luyện mạng\n",
    "nn = NeuralNetwork(input_size=784, hidden_size=128, output_size=10, learning_rate=0.25)\n",
    "nn.train(x_train, y_train, x_test, y_test, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0262c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_info = {\"weights\": [nn.W1.tolist(), nn.W2.tolist()],\n",
    "           \"biases\": [nn.b1.tolist(), nn.b2.tolist()],\n",
    "           \"learning_rate\": nn.lr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddc90b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu thông tin mạng vào file JSON\n",
    "with open('nn_info.json', 'w') as f:\n",
    "    json.dump(nn_info, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d80fb7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, W1, b1, W2, b2):\n",
    "    z1 = np.dot(x, W1) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = softmax(z2)\n",
    "    return a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c9323ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('nn_info.json', 'r') as f:\n",
    "    model_data = json.load(f)\n",
    "    # Chọn bộ trọng số đầu tiên\n",
    "W1 = np.array(model_data['weights'][0], dtype=np.float32)  # [784, 128]\n",
    "W2 = np.array(model_data['weights'][1], dtype=np.float32)  # [128, 10]\n",
    "b1 = np.array(model_data['biases'][0], dtype=np.float32).reshape(1, -1)\n",
    "b2 = np.array(model_data['biases'][1], dtype=np.float32).reshape(1, -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
